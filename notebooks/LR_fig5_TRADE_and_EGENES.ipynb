{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46191447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir('../')\n",
    "\n",
    "# Setting\n",
    "data_dir = 'data'\n",
    "ds_info = pd.read_csv('resources/datasets.csv')\n",
    "datasets = ds_info['file']\n",
    "dataset_names = ds_info['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987984c-ea1b-49d1-bf9c-96bc3cdc33f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Egene analysis calculation\n",
    "\n",
    "from src.preprocess import get_data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse import issparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "# === Parameters ===\n",
    "top_k = 30\n",
    "save_dir = \"egene_analysis\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Load your gene index helper ===\n",
    "def get_gene_index(gene_name, gene_names):\n",
    "    idx = np.where(gene_names == gene_name)[0]\n",
    "    return int(idx[0]) if idx.size > 0 else None\n",
    "\n",
    "\n",
    "# === Loop over datasets ===\n",
    "for ds in datasets:\n",
    "    data_path = os.path.join(data_dir, ds)\n",
    "    dataset_name = os.path.basename(data_path).replace(\".h5ad\", \"\")\n",
    "    print(f\"\\nProcessing {dataset_name}\")\n",
    "\n",
    "    # === Load data and get all perturbations\n",
    "    adata, X0, X1 = get_data(0,data_path)\n",
    "\n",
    "    adata.obs['perturbation_base'] = adata.obs['perturbation'].str.replace(r'g\\d+$', '', regex=True)\n",
    "    perturbations = adata.obs['perturbation_base'].unique()\n",
    "    perturbations = [p for p in perturbations if p != 'control']\n",
    "\n",
    "    # === Compute covariance of control\n",
    "    Sigma = np.cov(X0, rowvar=False)\n",
    "    eigvals, eigvecs = eigh(Sigma)\n",
    "    sorted_idx = np.argsort(eigvals)[::-1]\n",
    "    eigvecs_sorted = eigvecs[:, sorted_idx[:top_k]]\n",
    "    eigvals_sorted = eigvals[sorted_idx[:top_k]]\n",
    "\n",
    "    alpha_squared_records = []\n",
    "\n",
    "    for pert in perturbations:\n",
    "        selected_pert_data = adata[adata.obs['perturbation_base'] == pert]\n",
    "        if selected_pert_data.shape[0] < 2:\n",
    "            print(f\"Skipping {pert} (too few samples)\")\n",
    "            continue\n",
    "\n",
    "        X1 = selected_pert_data.X.toarray() if issparse(selected_pert_data.X) else selected_pert_data.X\n",
    "        delta_X = np.mean(X1, axis=0) - np.mean(X0, axis=0)\n",
    "\n",
    "        alpha = eigvecs_sorted.T @ delta_X\n",
    "        alpha_squared = alpha**2\n",
    "        alpha_squared_records.append({\n",
    "            \"perturbation\": pert,\n",
    "            \"alpha_squared\": alpha_squared\n",
    "        })\n",
    "\n",
    "\n",
    "    # === Save projection data ===\n",
    "    alpha_save_path = os.path.join(save_dir, f\"{dataset_name}_alpha_squared.pkl\")\n",
    "    with open(alpha_save_path, \"wb\") as f:\n",
    "        pickle.dump(alpha_squared_records, f)\n",
    "\n",
    "    print(f\"Saved results for {dataset_name} to {alpha_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daaee80-b91c-4046-a694-c1efddc0a6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b8e25-3b55-4e89-b708-382ffd88b173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82ec35-d70f-4e5b-b227-eb7bade4a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIG 5B , FIG S5B heatmaps\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# === Parameters ===\n",
    "top_k = 10  # Only use top 10 eigenmodes\n",
    "save_dir = \"egene_analysis\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# === Loop through datasets ===\n",
    "for ds in datasets:\n",
    "    data_path = os.path.join(data_dir, ds)\n",
    "    dataset_name = os.path.basename(data_path).replace(\".h5ad\", \"\")\n",
    "    alpha_path = os.path.join(save_dir, f\"{dataset_name}_alpha_squared.pkl\")\n",
    "\n",
    "    if not os.path.exists(alpha_path):\n",
    "        print(f\"Skipping {dataset_name}, missing alpha_squared.pkl\")\n",
    "        continue\n",
    "\n",
    "    with open(alpha_path, \"rb\") as f:\n",
    "        alpha_squared_records = pickle.load(f)\n",
    "\n",
    "    # Extract top-k components\n",
    "    perturbations = []\n",
    "    alpha_matrix_list = []\n",
    "    for record in alpha_squared_records:\n",
    "        alpha_sq = np.array(record[\"alpha_squared\"])\n",
    "        if len(alpha_sq) < top_k:\n",
    "            continue\n",
    "        alpha_matrix_list.append(alpha_sq[:top_k])\n",
    "        perturbations.append(record[\"perturbation\"])\n",
    "\n",
    "    if not alpha_matrix_list:\n",
    "        print(f\"No valid perturbations found for {dataset_name}\")\n",
    "        continue\n",
    "\n",
    "    alpha_matrix = np.array(alpha_matrix_list).T  # shape: (top_k, num_perts)\n",
    "    alpha_matrix_norm = alpha_matrix / np.sum(alpha_matrix, axis=0, keepdims=True)\n",
    "\n",
    "    # === Clustered heatmap with dendrograms ===\n",
    "    g = sns.clustermap(\n",
    "        alpha_matrix_norm,\n",
    "        cmap=\"viridis\",\n",
    "        row_cluster=True,\n",
    "        col_cluster=True,\n",
    "        xticklabels=perturbations,\n",
    "        yticklabels=[f\"PC{i+1}\" for i in range(top_k)],\n",
    "        figsize=(14, 8),\n",
    "        dendrogram_ratio=(0.2, 0.2),  # show row and col dendrograms\n",
    "        cbar_pos=(0.02, 0.8, 0.02, 0.18)  # standard colorbar\n",
    "    )\n",
    "    g.fig.suptitle(\n",
    "        rf\"{dataset_name}: Clustered Heatmap of Normalized $\\alpha_{{ij}}^2 / \\sum_i \\alpha_{{ij}}^2$\",\n",
    "        y=1.02\n",
    "    )\n",
    "    plt.setp(g.ax_heatmap.get_xticklabels(), rotation=90)\n",
    "\n",
    "    # === Save ===\n",
    "    heatmap_path = os.path.join(save_dir, f\"{dataset_name}_alpha_squared_clustermap_top{top_k}.svg\")\n",
    "    plt.savefig(heatmap_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved clustered heatmap: {heatmap_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b637cc94-9a55-45ef-87d6-ec869d4d395e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13cb8be-d02c-47b8-aa38-16b72ebbe98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644cd1ff-4b17-4788-8111-e467fdee20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIG 5D\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# === Paths ===\n",
    "alpha_dir = \"egene_analysis\"\n",
    "r2_dir = \"output/r2_histograms\"\n",
    "output_dir = \"FIG_TRADE\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Load mean and median participation ratios ===\n",
    "pr_stats = {}\n",
    "for file in os.listdir(alpha_dir):\n",
    "    if file.endswith(\"_alpha_squared.pkl\"):\n",
    "        dataset = file.replace(\"_alpha_squared.pkl\", \"\")\n",
    "        alpha_path = os.path.join(alpha_dir, file)\n",
    "\n",
    "        with open(alpha_path, \"rb\") as f:\n",
    "            alpha_squared_records = pickle.load(f)\n",
    "\n",
    "        participation_ratios = []\n",
    "        for rec in alpha_squared_records:\n",
    "            alpha_sq = np.array(rec[\"alpha_squared\"])\n",
    "            numerator = np.sum(alpha_sq) ** 2\n",
    "            denominator = np.sum(alpha_sq ** 2)\n",
    "            PR = numerator / denominator\n",
    "            participation_ratios.append(PR)\n",
    "\n",
    "        if participation_ratios:\n",
    "            pr_stats[dataset] = {\n",
    "                \"mean_pr\": np.mean(participation_ratios),\n",
    "                \"median_pr\": np.median(participation_ratios)\n",
    "            }\n",
    "\n",
    "# === Load mean RÂ² ===\n",
    "r2_means = {}\n",
    "for file in os.listdir(r2_dir):\n",
    "    if file.endswith(\"_results.csv\"):\n",
    "        dataset = file.replace(\"_results.csv\", \"\")\n",
    "        df = pd.read_csv(os.path.join(r2_dir, file))\n",
    "        if \"R2_real\" in df:\n",
    "            r2_means[dataset] = np.mean(df[\"R2_real\"].dropna())\n",
    "\n",
    "# === Merge summaries ===\n",
    "rows = []\n",
    "for dataset in pr_stats:\n",
    "    if dataset in r2_means:\n",
    "        rows.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"mean_pr\": pr_stats[dataset][\"mean_pr\"],\n",
    "            \"median_pr\": pr_stats[dataset][\"median_pr\"],\n",
    "            \"mean_r2\": r2_means[dataset]\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "# === Function to plot and fit line ===\n",
    "def plot_with_fit(x, y, labels, xlabel, ylabel, title, filename, color_palette):\n",
    "    X = np.array(x).reshape(-1, 1)\n",
    "    Y = np.array(y)\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X, Y)\n",
    "    Y_pred = reg.predict(X)\n",
    "    r2 = r2_score(Y, Y_pred)\n",
    "    slope = reg.coef_[0]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.scatter(x[i], y[i], color=color_palette[i], s=80, label=label)\n",
    "        plt.text(x[i], y[i], label, fontsize=9, ha='left', va='bottom')\n",
    "\n",
    "    plt.plot(x, Y_pred, color='black', linestyle='--', label=f\"Fit (RÂ² = {r2:.2f}, Slope = {slope:.2f})\")\n",
    "    plt.xlabel(xlabel, fontsize=14)\n",
    "    plt.ylabel(ylabel, fontsize=14)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=9, loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, filename))\n",
    "    plt.close()\n",
    "\n",
    "# === Generate color palette ===\n",
    "palette = plt.cm.tab10.colors if len(summary_df) <= 10 else plt.cm.tab20.colors\n",
    "\n",
    "# === Plot Mean PR vs Mean RÂ² ===\n",
    "plot_with_fit(\n",
    "    x=summary_df[\"mean_r2\"],\n",
    "    y=summary_df[\"mean_pr\"],\n",
    "    labels=summary_df[\"dataset\"],\n",
    "    xlabel=\"Mean RÂ² (Real Î£)\",\n",
    "    ylabel=\"Mean Participation Ratio\",\n",
    "    title=\"Mean PR vs Mean RÂ² Across Datasets\",\n",
    "    filename=\"mean_PR_vs_mean_R2.svg\",\n",
    "    color_palette=palette[:len(summary_df)]\n",
    ")\n",
    "\n",
    "# === Plot Median PR vs Mean RÂ² ===\n",
    "plot_with_fit(\n",
    "    x=summary_df[\"mean_r2\"],\n",
    "    y=summary_df[\"median_pr\"],\n",
    "    labels=summary_df[\"dataset\"],\n",
    "    xlabel=\"Mean RÂ² (Real Î£)\",\n",
    "    ylabel=\"Median Participation Ratio\",\n",
    "    title=\"Median PR vs Mean RÂ² Across Datasets\",\n",
    "    filename=\"median_PR_vs_mean_R2.svg\",\n",
    "    color_palette=palette[:len(summary_df)]\n",
    ")\n",
    "\n",
    "# === Save data table ===\n",
    "summary_df.to_csv(os.path.join(output_dir, \"PR_vs_R2_summary.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c43b7-3061-43d1-8f39-945490090e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d28b7e-14bb-4a10-8c0d-f315a031d066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289c75d-9e3d-4534-9ac4-308fa642627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIG 5 A, FIG S5A\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# === Parameters ===\n",
    "save_dir = \"egene_analysis\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Loop through datasets ===\n",
    "for ds in datasets:\n",
    "    data_path = os.path.join(data_dir, ds)\n",
    "    dataset_name = os.path.basename(data_path).replace(\".h5ad\", \"\")\n",
    "    alpha_path = os.path.join(save_dir, f\"{dataset_name}_alpha_squared.pkl\")\n",
    "\n",
    "    if not os.path.exists(alpha_path):\n",
    "        print(f\"Skipping {dataset_name}, missing alpha_squared.pkl\")\n",
    "        continue\n",
    "\n",
    "    with open(alpha_path, \"rb\") as f:\n",
    "        alpha_squared_records = pickle.load(f)\n",
    "\n",
    "    participation_ratios = []\n",
    "\n",
    "    for record in alpha_squared_records:\n",
    "        alpha_squared = np.array(record[\"alpha_squared\"])\n",
    "        numerator = np.sum(alpha_squared) ** 2\n",
    "        denominator = np.sum(alpha_squared**2)\n",
    "        PR = numerator / denominator\n",
    "        participation_ratios.append(PR)\n",
    "\n",
    "    if not participation_ratios:\n",
    "        print(f\"No valid alpha_squared data for {dataset_name}\")\n",
    "        continue\n",
    "\n",
    "    # === Plot histogram for this dataset ===\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(participation_ratios, bins=20, color='skyblue', edgecolor='k', alpha=0.8, density = True)\n",
    "    plt.xlabel(\"Participation Ratio\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"Distribution of PR â {dataset_name}\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === Save plot ===\n",
    "    hist_path = os.path.join(save_dir, f\"{dataset_name}_participation_ratio_hist.svg\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved PR histogram: {hist_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55791cc-c489-4b94-af7e-6fed386d02d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e505041-0746-4b19-b018-8aaba0ec1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIG S5C\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# === Parameters ===\n",
    "save_dir = \"egene_analysis\"\n",
    "output_dir = \"FIG_TRADE\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Define Tian datasets ===\n",
    "tian_datasets = {\n",
    "    \"TianKampmann2021_CRISPRa\",\n",
    "    \"TianKampmann2021_CRISPRi\",\n",
    "    \"TianKampmann2019_day7neuron\"\n",
    "}\n",
    "\n",
    "# === Collect PRs ===\n",
    "all_prs = []\n",
    "tian_prs = []\n",
    "\n",
    "for ds in datasets:\n",
    "    data_path = os.path.join(data_dir, ds)\n",
    "    dataset_name = os.path.basename(data_path).replace(\".h5ad\", \"\")\n",
    "    alpha_path = os.path.join(save_dir, f\"{dataset_name}_alpha_squared.pkl\")\n",
    "\n",
    "    if not os.path.exists(alpha_path):\n",
    "        print(f\"Skipping {dataset_name}, missing alpha_squared.pkl\")\n",
    "        continue\n",
    "\n",
    "    with open(alpha_path, \"rb\") as f:\n",
    "        alpha_squared_records = pickle.load(f)\n",
    "\n",
    "    for record in alpha_squared_records:\n",
    "        alpha_squared = np.array(record[\"alpha_squared\"])\n",
    "        numerator = np.sum(alpha_squared) ** 2\n",
    "        denominator = np.sum(alpha_squared ** 2)\n",
    "        PR = numerator / denominator\n",
    "\n",
    "        all_prs.append(PR)\n",
    "        if dataset_name in tian_datasets:\n",
    "            tian_prs.append(PR)\n",
    "\n",
    "# === Plotting function ===\n",
    "def plot_pr_histogram(prs, label, fname):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(prs, bins=40, density=True, alpha=0.85, color=\"steelblue\", edgecolor=\"k\")\n",
    "    plt.title(f\"Participation Ratio Distribution â {label}\")\n",
    "    plt.xlabel(\"Participation Ratio\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, fname))\n",
    "    plt.close()\n",
    "\n",
    "# === Plot: All Datasets ===\n",
    "plot_pr_histogram(all_prs, \"All Datasets\", \"PR_hist_all.svg\")\n",
    "\n",
    "# === Plot: Tian Datasets Combined ===\n",
    "plot_pr_histogram(tian_prs, \"Tian Datasets (CRISPRa/i + Day7Neuron)\", \"PR_hist_Tian.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0b0fd-1272-4e11-92e9-c22b2399a939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7b481-3e16-4ff0-8087-82e42c526d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0aa2b-5f9a-4709-8ca6-e7845431593a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ff033-354f-45a3-8ca8-5d7b91b30253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# participation ratio stats\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# === Parameters ===\n",
    "save_dir = \"egene_analysis\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "all_participation_ratios = []\n",
    "\n",
    "# === Loop through datasets ===\n",
    "for ds in datasets:\n",
    "    data_path = os.path.join(data_dir, ds)\n",
    "    dataset_name = os.path.basename(data_path).replace(\".h5ad\", \"\")\n",
    "    alpha_path = os.path.join(save_dir, f\"{dataset_name}_alpha_squared.pkl\")\n",
    "\n",
    "    if not os.path.exists(alpha_path):\n",
    "        print(f\"Skipping {dataset_name}, missing alpha_squared.pkl\")\n",
    "        continue\n",
    "\n",
    "    with open(alpha_path, \"rb\") as f:\n",
    "        alpha_squared_records = pickle.load(f)\n",
    "\n",
    "    participation_ratios = []\n",
    "\n",
    "    for record in alpha_squared_records:\n",
    "        alpha_squared = np.array(record[\"alpha_squared\"])\n",
    "        numerator = np.sum(alpha_squared) ** 2\n",
    "        denominator = np.sum(alpha_squared**2)\n",
    "        PR = numerator / denominator\n",
    "        participation_ratios.append(PR)\n",
    "\n",
    "    if not participation_ratios:\n",
    "        print(f\"No valid alpha_squared data for {dataset_name}\")\n",
    "        continue\n",
    "\n",
    "    # Append to global list\n",
    "    all_participation_ratios.extend(participation_ratios)\n",
    "\n",
    "   \n",
    "# === Compute and report summary statistics ===\n",
    "all_participation_ratios = np.array(all_participation_ratios)\n",
    "mean_pr = np.mean(all_participation_ratios)\n",
    "stderr_pr = np.std(all_participation_ratios, ddof=1) / np.sqrt(len(all_participation_ratios))\n",
    "\n",
    "print(\"\\nð Overall Participation Ratio Summary\")\n",
    "print(f\"Average PR across all datasets: {mean_pr:.2f}\")\n",
    "print(f\"Standard Error of the Mean (SEM): {stderr_pr:.2f}\")\n",
    "print(f\"Total perturbations analyzed: {len(all_participation_ratios)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f394c14-cabd-42de-ae2d-25b074a9482f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a6c66-0bac-4735-ac3e-b66debc38cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GO heatmap analysis\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import pickle\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse import issparse\n",
    "from gprofiler import GProfiler\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Parameters ===\n",
    "top_k = 30\n",
    "top_n_genes = 200\n",
    "save_dir = \"egene_analysis\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "gp = GProfiler(return_dataframe=True)\n",
    "\n",
    "# === Loop over datasets ===\n",
    "for ds in datasets:\n",
    "    data_path = os.path.join(data_dir, ds)\n",
    "    dataset_name = os.path.basename(data_path).replace(\".h5ad\", \"\")\n",
    "    print(f\"\\nð Processing {dataset_name}\")\n",
    "\n",
    "    try:\n",
    "        adata, X0, _ = get_data(0, data_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {dataset_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"perturbation\" not in adata.obs:\n",
    "        print(f\"No 'perturbation' column in {dataset_name}\")\n",
    "        continue\n",
    "\n",
    "    # Compute covariance\n",
    "    Sigma = np.cov(X0, rowvar=False)\n",
    "    eigvals, eigvecs = eigh(Sigma)\n",
    "\n",
    "    # Sort by descending eigenvalue\n",
    "    sorted_idx = np.argsort(eigvals)[::-1]\n",
    "    eigvals_sorted = eigvals[sorted_idx[:top_k]]\n",
    "    eigvecs_sorted = eigvecs[:, sorted_idx[:top_k]]\n",
    "\n",
    "    gene_names = np.array(adata.var_names)\n",
    "\n",
    "    # === Save eigenvectors and gene names ===\n",
    "    np.save(os.path.join(save_dir, f\"{dataset_name}_eigvecs_sorted.npy\"), eigvecs_sorted)\n",
    "    np.save(os.path.join(save_dir, f\"{dataset_name}_gene_names.npy\"), gene_names)\n",
    "\n",
    "    # === Get top contributing genes for each eigengene ===\n",
    "    eigengene_to_genes = {}\n",
    "    for i in range(top_k):\n",
    "        loading_vector = eigvecs_sorted[:, i]\n",
    "        top_indices = np.argsort(-np.abs(loading_vector))[:top_n_genes]\n",
    "        top_gene_names = gene_names[top_indices].tolist()\n",
    "        eig_label = f\"Eig {i+1}\"\n",
    "        eigengene_to_genes[eig_label] = top_gene_names\n",
    "\n",
    "    # === Run GO enrichment ===\n",
    "    enrichment_results = {}\n",
    "    for eig_label, gene_list in eigengene_to_genes.items():\n",
    "        try:\n",
    "            result_df = gp.profile(\n",
    "                organism=\"hsapiens\",\n",
    "                query=gene_list,\n",
    "                sources=[\"GO:BP\", \"GO:MF\", \"REAC\"],\n",
    "                user_threshold=0.05,\n",
    "                no_evidences=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"gProfiler failed on {eig_label} in {dataset_name}: {e}\")\n",
    "            result_df = pd.DataFrame()\n",
    "\n",
    "        if not result_df.empty:\n",
    "            top_terms = result_df[['name', 'p_value']].sort_values(by='p_value').head(5)\n",
    "            enrichment_results[eig_label] = top_terms\n",
    "        else:\n",
    "            enrichment_results[eig_label] = None\n",
    "\n",
    "    # === Build GO term matrix ===\n",
    "    go_term_dict = defaultdict(dict)\n",
    "    for eig, df in enrichment_results.items():\n",
    "        if df is None:\n",
    "            continue\n",
    "        for _, row in df.iterrows():\n",
    "            term = row[\"name\"]\n",
    "            score = -np.log10(row[\"p_value\"] + 1e-300)\n",
    "            go_term_dict[term][eig] = score\n",
    "\n",
    "    go_term_df = pd.DataFrame(go_term_dict).T.fillna(0)\n",
    "\n",
    "    # === Save results ===\n",
    "    with open(os.path.join(save_dir, f\"{dataset_name}_go_enrichment.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(enrichment_results, f)\n",
    "\n",
    "    go_term_df.to_csv(os.path.join(save_dir, f\"{dataset_name}_go_matrix.csv\"))\n",
    "\n",
    "    print(f\"Finished {dataset_name}: saved eigvecs, enrichment, and matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0a981-4422-4626-b3bf-1a7dc69f6ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d474d99-1d76-4550-9a89-d17ab92793ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c667d-00d4-4b84-bbaa-0d1d89fb2e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8d235-fada-4f00-a61a-0331780cda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIG 5 C\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Parameters ===\n",
    "save_dir = \"egene_analysis\"\n",
    "top_terms_to_plot = 6\n",
    "\n",
    "\n",
    "# === Loop over datasets and generate plots ===\n",
    "for dataset_name in dataset_names:\n",
    "    go_matrix_path = os.path.join(save_dir, f\"{dataset_name}_go_matrix.csv\")\n",
    "\n",
    "    if not os.path.exists(go_matrix_path):\n",
    "        print(f\"Missing matrix: {go_matrix_path}\")\n",
    "        continue\n",
    "\n",
    "    go_term_df = pd.read_csv(go_matrix_path, index_col=0)\n",
    "\n",
    "    if go_term_df.empty:\n",
    "        print(f\"Empty GO matrix for {dataset_name}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # === Select top enriched terms (by max enrichment across any eigengene) ===\n",
    "    top_terms = go_term_df.max(axis=1).sort_values(ascending=False).head(top_terms_to_plot).index\n",
    "    filtered_df = go_term_df.loc[top_terms]\n",
    "\n",
    "    # === Plot ===\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(filtered_df, cmap=\"magma\", cbar_kws={\"label\": \"-log10(p-value)\"})\n",
    "    plt.title(f\"{dataset_name} â GO Enrichment Across Eigengenes\")\n",
    "    plt.xlabel(\"Eigengene\")\n",
    "    plt.ylabel(\"GO Term\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(save_dir, f\"{dataset_name}_go_heatmap.svg\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved heatmap: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b17b86-4227-4874-9d09-26767f654c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined datase4t GO analysis\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gprofiler import GProfiler\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Parameters ===\n",
    "top_n_genes = 200\n",
    "top_terms_to_plot = 25\n",
    "top_k = 30\n",
    "save_dir = \"egene_analysis\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "gp = GProfiler(return_dataframe=True)\n",
    "\n",
    "# === Dataset groups ===\n",
    "groups = {\n",
    "    \"Tian_CRISPR_combined\": [\n",
    "        \"TianKampmann2021_CRISPRi\",\n",
    "        \"TianKampmann2021_CRISPRa\",\n",
    "        \"TianKampmann2019_day7neuron\"\n",
    "    ],\n",
    "    \"All\": dataset_names\n",
    "}\n",
    "\n",
    "# === Load top loading genes per PC per dataset ===\n",
    "def get_top_genes_by_pc(dataset):\n",
    "    eigvec_path = os.path.join(save_dir, f\"{dataset}_eigvecs_sorted.npy\")\n",
    "    genes_path = os.path.join(save_dir, f\"{dataset}_gene_names.npy\")\n",
    "\n",
    "    if not (os.path.exists(eigvec_path) and os.path.exists(genes_path)):\n",
    "        print(f\"Missing eigenvectors or gene names for {dataset}\")\n",
    "        return None\n",
    "\n",
    "    eigvecs_sorted = np.load(eigvec_path)\n",
    "    gene_names = np.load(genes_path, allow_pickle=True)\n",
    "    pc_to_genes = {}\n",
    "\n",
    "    for i in range(min(top_k, eigvecs_sorted.shape[1])):\n",
    "        loading_vector = eigvecs_sorted[:, i]\n",
    "        top_indices = np.argsort(-np.abs(loading_vector))[:top_n_genes]\n",
    "        top_genes = set(gene_names[top_indices])\n",
    "        pc_to_genes[f\"Eig {i+1}\"] = top_genes\n",
    "\n",
    "    return pc_to_genes\n",
    "\n",
    "# === Run GO enrichment on intersected genes per PC ===\n",
    "def run_go_heatmap_for_group(group_name, dataset_list):\n",
    "    print(f\"\\nð Processing group: {group_name}\")\n",
    "    all_pc_to_genes = [get_top_genes_by_pc(ds) for ds in dataset_list]\n",
    "\n",
    "    if any(x is None for x in all_pc_to_genes):\n",
    "        print(f\"Skipping {group_name} due to missing data\")\n",
    "        return\n",
    "\n",
    "    # Keys are guaranteed to be aligned: Eig 1 ... Eig K\n",
    "    all_pcs = all_pc_to_genes[0].keys()\n",
    "    go_term_dict = defaultdict(dict)\n",
    "\n",
    "    for pc in all_pcs:\n",
    "        gene_sets = [d[pc] for d in all_pc_to_genes if pc in d]\n",
    "        if len(gene_sets) < 2:\n",
    "            continue\n",
    "        shared_genes = set.intersection(*gene_sets)\n",
    "        if len(shared_genes) < 5:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result_df = gp.profile(\n",
    "                organism=\"hsapiens\",\n",
    "                query=list(shared_genes),\n",
    "                sources=[\"GO:BP\", \"GO:MF\", \"REAC\"],\n",
    "                user_threshold=0.05,\n",
    "                no_evidences=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"gProfiler failed on {group_name} {pc}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if not result_df.empty:\n",
    "            top_terms = result_df[['name', 'p_value']].sort_values(by='p_value').head(top_terms_to_plot)\n",
    "            for _, row in top_terms.iterrows():\n",
    "                term = row[\"name\"]\n",
    "                score = -np.log10(row[\"p_value\"] + 1e-300)\n",
    "                go_term_dict[term][pc] = score\n",
    "\n",
    "    # === Assemble and save GO matrix ===\n",
    "    go_df = pd.DataFrame(go_term_dict).T.fillna(0)\n",
    "\n",
    "    if go_df.empty:\n",
    "        print(f\"No significant enrichment for {group_name}\")\n",
    "        return\n",
    "\n",
    "    # Reduce to top overall terms\n",
    "    top_rows = go_df.max(axis=1).sort_values(ascending=False).head(top_terms_to_plot).index\n",
    "    filtered_df = go_df.loc[top_rows]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(1 + 0.4 * len(filtered_df.columns), 0.5 * len(filtered_df)))\n",
    "    sns.heatmap(filtered_df, cmap=\"magma\", cbar_kws={\"label\": \"-log10(p-value)\"})\n",
    "    plt.title(f\"{group_name} â Shared Top Gene GO Enrichment per PC\")\n",
    "    plt.xlabel(\"Eigengene\")\n",
    "    plt.ylabel(\"GO Term\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(save_dir, f\"{group_name}_GO_PC_heatmap.svg\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Save matrix\n",
    "    csv_path = os.path.join(save_dir, f\"{group_name}_GO_PC_matrix.csv\")\n",
    "    filtered_df.to_csv(csv_path)\n",
    "\n",
    "    print(f\"Saved: {plot_path}, {csv_path}\")\n",
    "\n",
    "# === Run for all groups ===\n",
    "for group_name, dataset_list in groups.items():\n",
    "    run_go_heatmap_for_group(group_name, dataset_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d55bf-3292-4876-8657-0e1b425990b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aafec00-6b16-499f-bd68-2c17fba95250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e697b-2dca-4191-8ae2-7a0bde0b8cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525451e1-e4b6-416b-b38b-6b68a040f6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b080c-f989-4b2f-a2fb-30821703ce26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f661b-7024-4760-8166-e461f26059b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e202a46-4a7c-430b-9ad8-70d6d7357108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b79aea-1bd7-40af-b1ca-d32290574e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b2f8a-0a3f-423a-a71d-1cb42bba3189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d985d-13b5-4a07-8338-e9e2d37b0ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fd452-d9fe-4b10-a06b-294df2c83371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36daea-356c-4cbc-9094-92041b50adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRADE comparison\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.linalg import pinv\n",
    "from scipy.sparse import issparse\n",
    "from scipy.stats import entropy, skew, kurtosis\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_gene_contributions(adata, X0_dense, Sigma, gene_names):\n",
    "    def get_gene_index(gene_name):\n",
    "        idx = np.where(gene_names == gene_name)[0]\n",
    "        return int(idx[0]) if idx.size > 0 else None\n",
    "\n",
    "    def compute_average_response(X0, X1):\n",
    "        return X1.mean(axis=0) - X0.mean(axis=0)\n",
    "\n",
    "    def compute_macro_metrics(frac_vector_global, frac_row, delta_X, u_star, gene_index):\n",
    "        sorted_contribs = np.sort(frac_vector_global)[::-1]\n",
    "        cumsum = np.cumsum(sorted_contribs)\n",
    "        thresholds = [0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "        genes_needed = {f\"genes_for_{int(t*100)}pct\": int(np.searchsorted(cumsum, t) + 1) for t in thresholds}\n",
    "        eff_size_global = np.exp(entropy(frac_vector_global + 1e-12))\n",
    "        eff_size_target = np.exp(entropy(frac_row + 1e-12))\n",
    "        cos_sim = (u_star @ delta_X) / (np.linalg.norm(u_star) * np.linalg.norm(delta_X) + 1e-12)\n",
    "        skw = skew(frac_vector_global)\n",
    "        krt = kurtosis(frac_vector_global)\n",
    "        top_contribs = sorted_contribs[:10]\n",
    "        log_ranks = np.log(np.arange(1, 11))\n",
    "        log_contribs = np.log(top_contribs + 1e-12)\n",
    "        slope = np.polyfit(log_ranks, log_contribs, 1)[0] if len(log_contribs) == 10 else np.nan\n",
    "        return {\n",
    "            **genes_needed,\n",
    "            \"eff_size_global\": eff_size_global,\n",
    "            \"eff_size_target\": eff_size_target,\n",
    "            \"eff_size_diff\": eff_size_target - eff_size_global,\n",
    "            \"cosine_similarity\": cos_sim,\n",
    "            \"skewness\": skw,\n",
    "            \"kurtosis\": krt,\n",
    "            \"zipf_slope\": slope\n",
    "        }\n",
    "\n",
    "    def compute_self_rank(frac_row, gene_index):\n",
    "        ranked = np.argsort(frac_row)[::-1]\n",
    "        rank = np.where(ranked == gene_index)[0]\n",
    "        return int(rank[0]) + 1 if rank.size > 0 else np.nan\n",
    "\n",
    "    Sigma_inv = pinv(Sigma)\n",
    "    abs_Sigma = np.abs(Sigma)\n",
    "    perturbation_counts = adata.obs['perturbation'].value_counts()\n",
    "    perturbations = [p for p in perturbation_counts.index if p != 'control']\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for pert in perturbations:\n",
    "        gene_index = get_gene_index(pert)\n",
    "        if gene_index is None:\n",
    "            continue\n",
    "\n",
    "        X1 = adata[adata.obs['perturbation'] == pert].X\n",
    "        X1 = X1.toarray() if issparse(X1) else X1\n",
    "        delta_X = compute_average_response(X0_dense, X1)\n",
    "        u_star = Sigma_inv @ delta_X\n",
    "\n",
    "        abs_u = np.abs(u_star)\n",
    "        contrib_sums = abs_Sigma @ abs_u\n",
    "        frac_vector_global = contrib_sums / (np.sum(contrib_sums) + 1e-8)\n",
    "\n",
    "        row = abs_Sigma[gene_index, :] * abs_u\n",
    "        frac_row = row / (np.sum(row) + 1e-8)\n",
    "\n",
    "        metrics_dict = compute_macro_metrics(frac_vector_global, frac_row, delta_X, u_star, gene_index)\n",
    "        metrics_dict[\"Perturbation\"] = pert\n",
    "        metrics_dict[\"self_rank\"] = compute_self_rank(frac_row, gene_index)\n",
    "\n",
    "        metrics.append(metrics_dict)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "save_dir = \"contribution_outputs\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for ds in datasets:\n",
    "    data_path = os.path.join(data_dir, ds)\n",
    "    print(data_path)\n",
    "    base = os.path.basename(data_path).replace(\".h5ad\", \"\")\n",
    "    adata, X0, _ = get_data(0, data_path)\n",
    "    gene_names = np.array(adata.var_names)\n",
    "    X0_dense = X0.toarray() if issparse(X0) else X0\n",
    "    Sigma = np.cov(X0_dense, rowvar=False)\n",
    "\n",
    "    metrics = analyze_gene_contributions(adata, X0_dense, Sigma, gene_names)\n",
    "\n",
    "    with open(os.path.join(save_dir, f\"{base}_macro_contribution.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(metrics, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e9630-1bfb-496e-a0ed-3beb39cc7d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIG 5 E/F FIG S5D\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# === Setup ===\n",
    "save_dir = \"contribution_outputs\"\n",
    "output_dir = \"FIG_TRADE\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Load all metric files ===\n",
    "all_metrics = []\n",
    "for file in os.listdir(save_dir):\n",
    "    if file.endswith(\"_macro_contribution.pkl\"):\n",
    "        with open(os.path.join(save_dir, file), \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "            df[\"dataset\"] = file.replace(\"_macro_contribution.pkl\", \"\")\n",
    "            all_metrics.append(df)\n",
    "\n",
    "combined_df = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "# === Descriptive labels ===\n",
    "label_dict = {\n",
    "    \"eff_size_global\": \"Number of Genes Driving Global Transcriptomic Change\",\n",
    "    \"eff_size_target\": \"Genes contriubuting to True Perturbed Gene's response\",\n",
    "    \"self_rank\": \"Rank of how much True Perturbed Gene affects its own change\",\n",
    "    \"genes_for_50pct\": \"Genes Explaining 50% of Total Response\",\n",
    "    \"genes_for_75pct\": \"Genes Explaining 75% of Total Response\",\n",
    "    \"genes_for_90pct\": \"Genes Explaining 90% of Total Response\",\n",
    "    \"genes_for_95pct\": \"Genes Explaining 95% of Total Response\",\n",
    "    \"genes_for_99pct\": \"Genes Explaining 99% of Total Response\"\n",
    "}\n",
    "\n",
    "metrics_to_plot = list(label_dict.keys())\n",
    "\n",
    "# === Plotting function ===\n",
    "def plot_metric_distribution(df, metric, log_scale=False):\n",
    "    label = label_dict.get(metric, metric)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    values = df[metric].dropna()\n",
    "    if log_scale:\n",
    "        bins = np.logspace(np.log10(values.min() + 1e-3), np.log10(values.max() + 1), 30)\n",
    "    else:\n",
    "        bins = 25\n",
    "\n",
    "    sns.histplot(values, bins=bins, kde=True, stat=\"density\", color=\"steelblue\", edgecolor=None, alpha=0.8)\n",
    "    plt.title(f\"Distribution of {label}\", fontsize=14)\n",
    "    plt.xlabel(label, fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    if log_scale:\n",
    "        plt.xscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"plot_{metric}.svg\"))\n",
    "    plt.close()\n",
    "\n",
    "# === Generate all plots ===\n",
    "for metric in metrics_to_plot:\n",
    "    logscale = metric in [\"eff_size_global\", \"eff_size_target\", \"self_rank\"]\n",
    "    plot_metric_distribution(combined_df, metric, log_scale=logscale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050fd9be-0b92-434e-bd38-2dacd7cd101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIG 5 E/F FIG S5D\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# === Setup ===\n",
    "save_dir = \"contribution_outputs\"\n",
    "output_dir = \"FIG_TRADE\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Load all metric files ===\n",
    "all_metrics = []\n",
    "for file in os.listdir(save_dir):\n",
    "    if file.endswith(\"_macro_contribution.pkl\"):\n",
    "        with open(os.path.join(save_dir, file), \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "            df[\"dataset\"] = file.replace(\"_macro_contribution.pkl\", \"\")\n",
    "            all_metrics.append(df)\n",
    "\n",
    "combined_df = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "# === Descriptive labels ===\n",
    "label_dict = {\n",
    "    \"eff_size_global\": \"Number of Genes Driving Global Transcriptomic Change\",\n",
    "    \"eff_size_target\": \"Genes Contributing to True Perturbed Gene's Response\",\n",
    "    \"self_rank\": \"Rank of True Perturbed Gene by Its Own Response Influence\",\n",
    "    # \"genes_for_50pct\": \"Genes Explaining 50% of Total Response\",\n",
    "    # \"genes_for_75pct\": \"Genes Explaining 75% of Total Response\",\n",
    "    # \"genes_for_90pct\": \"Genes Explaining 90% of Total Response\",\n",
    "    # \"genes_for_95pct\": \"Genes Explaining 95% of Total Response\",\n",
    "    \"genes_for_99pct\": \"Genes Explaining 99% of Total Response\"\n",
    "}\n",
    "\n",
    "metrics_to_plot = list(label_dict.keys())\n",
    "\n",
    "# === Plotting function ===\n",
    "def freedman_diaconis_bins(data):\n",
    "    \"\"\"Compute number of bins using the FreedmanâDiaconis rule.\"\"\"\n",
    "    data = np.asarray(data)\n",
    "    iqr = np.subtract(*np.percentile(data, [75, 25]))\n",
    "    bin_width = 2 * iqr / (len(data) ** (1 / 3) + 1e-8)\n",
    "    if bin_width == 0:\n",
    "        return 25  # fallback\n",
    "    return int(np.ceil((data.max() - data.min()) / bin_width))\n",
    "\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "def plot_metric_distribution(df, metric, num_bins=150):\n",
    "    values = df[metric].dropna()\n",
    "    label = label_dict.get(metric, metric)\n",
    "\n",
    "    # Determine log-scale use\n",
    "    skewness = values.skew()\n",
    "    value_range = values.max() / (values.min() + 1e-5)\n",
    "    use_log = skewness > 2 or value_range > 100\n",
    "\n",
    "    # Choose bins\n",
    "    if use_log:\n",
    "        bins = np.logspace(np.log10(values.min() + 1e-3), np.log10(values.max() + 1), num_bins)\n",
    "    else:\n",
    "        bins = num_bins\n",
    "\n",
    "    # Compute statistics\n",
    "    mean_val = values.mean()\n",
    "    kurt = kurtosis(values, fisher=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(values, bins=bins, kde=False, stat=\"density\", color=\"steelblue\", edgecolor=\"black\", alpha=0.85)\n",
    "\n",
    "    # Annotations\n",
    "    annotation_text = f\"Mean = {mean_val:.1f}\\nKurtosis = {kurt:.1f}\"\n",
    "    plt.annotate(annotation_text, xy=(0.98, 0.95), xycoords='axes fraction',\n",
    "                 ha='right', va='top', fontsize=11, bbox=dict(facecolor='white', edgecolor='gray', alpha=0.7))\n",
    "\n",
    "    # Labels and formatting\n",
    "    plt.xlabel(label, fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.title(f\"Distribution of {label}\", fontsize=14)\n",
    "    if use_log:\n",
    "        plt.xscale(\"log\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"plot_{metric}.svg\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Generate all plots ===\n",
    "for metric in metrics_to_plot:\n",
    "    plot_metric_distribution(combined_df, metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b118e-dcde-4761-8ae5-24730206d960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03f08a-cf5a-45cf-ab70-71b628f3e081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339dd2d-de3b-4f17-9b77-a0498e02d0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18270814-9c38-4dff-86ba-3c838e800052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e41821e-6d17-4de9-b07f-1c853c6c4f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904466b1-9716-4448-9235-07a18961efac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e540e-e252-4f92-b4fc-47e98a211577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a9c58-43ba-4cd0-8829-ef68c0c71658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all u vectors for further analysis\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.linalg import pinv\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "from anndata import read_h5ad\n",
    "\n",
    "\n",
    "save_dir = \"u_vectors\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "data_paths = [\n",
    "    \"TianKampmann2019_day7neuron.h5ad\",\n",
    "    \"ReplogleWeissman2022_rpe1.h5ad\",\n",
    "    \"ReplogleWeissman2022_K562_essential.h5ad\",\n",
    "    \"GSE264667_jurkat_raw_singlecell_01.h5ad\", \n",
    "    \"GSE264667_hepg2_raw_singlecell_01.h5ad\",\n",
    "    \"NormanWeissman2019_filtered.h5ad\",\n",
    "    \"FrangiehIzar2021_RNA.h5ad\",\n",
    "    \"TianKampmann2021_CRISPRi.h5ad\",\n",
    "    \"TianKampmann2021_CRISPRa.h5ad\",\n",
    "    \"TianKampmann2019_iPSC.h5ad\"\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    print(f\"Processing: {data_path}\")\n",
    "    base = os.path.basename(data_path).replace(\".h5ad\", \"\")\n",
    "    \n",
    "    try:\n",
    "        adata, X0, _ = get_data(0, data_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {base}: {e}\")\n",
    "        continue\n",
    "\n",
    "    Sigma = np.cov(X0, rowvar=False)\n",
    "    Sigma_inv = pinv(Sigma)\n",
    "\n",
    "    gene_names = np.array(adata.var_names)\n",
    "    pert_list = [p for p in adata.obs['perturbation'].unique() if p != \"control\"]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for pert in pert_list:\n",
    "        X1 = adata[adata.obs['perturbation'] == pert].X\n",
    "        if X1.shape[0] < 2:\n",
    "            continue\n",
    "        X1 = X1.toarray() if issparse(X1) else X1\n",
    "        delta_X = np.mean(X1, axis=0) - np.mean(X0, axis=0)\n",
    "        u_star = Sigma_inv @ delta_X\n",
    "\n",
    "        results.append({\n",
    "            \"perturbation\": pert,\n",
    "            \"u_star\": u_star,\n",
    "        })\n",
    "\n",
    "    with open(os.path.join(save_dir, f\"{base}_u_vectors.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    print(f\"Saved: {base}_u_vectors.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f71410-ca8f-4566-9dc5-0faff93d0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy of u\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# === Parameters ===\n",
    "save_dir = \"u_vectors\"\n",
    "fig_dir = \"u_entropy_figures\"\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# === Collect entropies and vectors ===\n",
    "all_entropies = []\n",
    "entropies_by_dataset = {}\n",
    "u_vectors_by_dataset = {}\n",
    "\n",
    "for file in os.listdir(save_dir):\n",
    "    if not file.endswith(\"_u_vectors.pkl\"):\n",
    "        continue\n",
    "    dataset = file.replace(\"_u_vectors.pkl\", \"\")\n",
    "    with open(os.path.join(save_dir, file), \"rb\") as f:\n",
    "        records = pickle.load(f)\n",
    "\n",
    "    entropies = []\n",
    "    vectors = []\n",
    "\n",
    "    for record in records:\n",
    "        u = np.abs(record[\"u_star\"])\n",
    "        p = u / (np.sum(u) + 1e-8)\n",
    "        ent = entropy(p)\n",
    "        entropies.append(ent)\n",
    "        vectors.append((record[\"perturbation\"], record[\"u_star\"], ent))\n",
    "\n",
    "    entropies_by_dataset[dataset] = entropies\n",
    "    u_vectors_by_dataset[dataset] = vectors\n",
    "    all_entropies.extend(entropies)\n",
    "\n",
    "# === Plot overall entropy histogram ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(all_entropies, bins=30, kde=True, stat=\"density\", color=\"teal\")\n",
    "plt.xlabel(\"Entropy of |u| Distribution\", fontsize=14)\n",
    "plt.ylabel(\"Density\", fontsize=14)\n",
    "plt.title(\"Distribution of Entropy Across All Perturbations\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(fig_dir, \"overall_entropy_histogram.svg\"))\n",
    "plt.close()\n",
    "\n",
    "# === Plot per-dataset entropy histograms ===\n",
    "for dataset, entropies in entropies_by_dataset.items():\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.histplot(entropies, bins=25, kde=True, stat=\"density\", color=\"steelblue\")\n",
    "    plt.xlabel(\"Entropy of |u|\", fontsize=13)\n",
    "    plt.ylabel(\"Density\", fontsize=13)\n",
    "    plt.title(f\"{dataset}: Entropy of Perturbation Vectors\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, f\"{dataset}_entropy_histogram.svg\"))\n",
    "    plt.close()\n",
    "\n",
    "# === Plot examples of high and low entropy perturbations ===\n",
    "for dataset, records in u_vectors_by_dataset.items():\n",
    "    if len(records) < 6:\n",
    "        continue\n",
    "\n",
    "    records_sorted = sorted(records, key=lambda x: x[2])  # sort by entropy\n",
    "    low_ents = records_sorted[:3]\n",
    "    high_ents = records_sorted[-3:]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 6), sharey=True)\n",
    "    for i, (pert, uvec, ent) in enumerate(low_ents):\n",
    "        axs[0, i].plot(uvec)\n",
    "        axs[0, i].set_title(f\"Low Entropy\\n{pert}\\nH={ent:.2f}\")\n",
    "    for i, (pert, uvec, ent) in enumerate(high_ents):\n",
    "        axs[1, i].plot(uvec)\n",
    "        axs[1, i].set_title(f\"High Entropy\\n{pert}\\nH={ent:.2f}\")\n",
    "\n",
    "    fig.suptitle(f\"{dataset} â Examples of High vs Low Entropy Perturbations\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(fig_dir, f\"{dataset}_u_vector_entropy_examples.svg\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9051d-9a97-4c73-965a-7edbf54eaa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy of u continued\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# === Paths ===\n",
    "save_dir = \"u_vectors\"\n",
    "fig_dir = \"u_entropy_figures\"\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# === Collect entropies by dataset ===\n",
    "entropies_by_dataset = {}\n",
    "\n",
    "for file in os.listdir(save_dir):\n",
    "    if not file.endswith(\"_u_vectors.pkl\"):\n",
    "        continue\n",
    "    dataset = file.replace(\"_u_vectors.pkl\", \"\")\n",
    "    with open(os.path.join(save_dir, file), \"rb\") as f:\n",
    "        records = pickle.load(f)\n",
    "\n",
    "    entropies = []\n",
    "    for record in records:\n",
    "        u = np.abs(record[\"u_star\"])\n",
    "        p = u / (np.sum(u) + 1e-8)\n",
    "        ent = entropy(p)\n",
    "        entropies.append(ent)\n",
    "\n",
    "    entropies_by_dataset[dataset] = entropies\n",
    "\n",
    "# === Overlaid entropy histograms ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "colors = sns.color_palette(\"hls\", len(entropies_by_dataset))\n",
    "\n",
    "for color, (dataset, entropies) in zip(colors, entropies_by_dataset.items()):\n",
    "    sns.kdeplot(entropies, fill=True, alpha=0.3, label=dataset, color=color, linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Entropy of |u| Distribution\", fontsize=14)\n",
    "plt.ylabel(\"Density\", fontsize=14)\n",
    "plt.title(\"Entropy of Perturbation Vectors Across Datasets\", fontsize=16)\n",
    "plt.legend(fontsize=9, loc=\"upper right\", ncol=1)\n",
    "plt.yscale('log')\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(fig_dir, \"entropy_histograms_overlay.svg\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098aa371-94c0-4cc8-a057-9a2ec1a78d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cipher",
   "language": "python",
   "name": "cipher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
