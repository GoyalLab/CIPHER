{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c611b2a-37c9-4418-9d41-7618a6bcfae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference first, then R2 calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "data_dir = 'data'\n",
    "out_dir = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ae9ca-ecc6-4da3-be35-8188d6e21a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the inference code \n",
    "# this saves many figures per perturbation (you can comment out if you want)\n",
    "from src.preprocess import get_data\n",
    "\n",
    "\n",
    "import pytensor\n",
    "pytensor.config.cxx = \"\"\n",
    "pytensor.config.mode = \"NUMBA\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import pinv\n",
    "from scipy.sparse import issparse\n",
    "from scipy.stats import nbinom\n",
    "import pymc as pm\n",
    "import os\n",
    "import pickle\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "def sample_zinb(mean, var, zero_prob, size):\n",
    "    if var <= mean:\n",
    "        var = mean + 1e-3\n",
    "    p = mean / var\n",
    "    r = mean**2 / (var - mean)\n",
    "    nb_samples = nbinom.rvs(n=r, p=p, size=size)\n",
    "    zeros = np.random.rand(size) < zero_prob\n",
    "    nb_samples[zeros] = 0\n",
    "    return nb_samples\n",
    "\n",
    "def compute_zinb_covariance(X0, n_cells=None, seed=42):\n",
    "    if issparse(X0):\n",
    "        X0 = X0.toarray()\n",
    "    np.random.seed(seed)\n",
    "    n_obs, n_genes = X0.shape\n",
    "    n_cells = n_obs if n_cells is None else n_cells\n",
    "    mu = X0.mean(axis=0)\n",
    "    var_emp = X0.var(axis=0)\n",
    "    zero_prob = (X0 == 0).mean(axis=0)\n",
    "    low_range = np.random.uniform(-1, -.5, size=n_genes // 2)\n",
    "    high_range = np.random.uniform(.5, 1, size=n_genes - len(low_range))\n",
    "    scaling_factors = 10**np.random.permutation(np.concatenate([low_range, high_range]))\n",
    "    target_var = scaling_factors * var_emp\n",
    "    synthetic_data = np.zeros((n_cells, n_genes))\n",
    "    for g in range(n_genes):\n",
    "        synthetic_data[:, g] = sample_zinb(mu[g], target_var[g], zero_prob[g], n_cells)\n",
    "    Sigma_zinb = np.cov(synthetic_data, rowvar=False)\n",
    "    return Sigma_zinb, synthetic_data, scaling_factors\n",
    "\n",
    "def compute_average_response(X0, X1):\n",
    "    return X1.mean(axis=0) - X0.mean(axis=0)\n",
    "\n",
    "def compute_sparse_perturbation(Sigma_inv, delta_X, gene_indices, top_k=200):\n",
    "    u_hat = Sigma_inv @ delta_X\n",
    "    sorted_indices = np.argsort(np.abs(u_hat))[::-1]\n",
    "    top_indices = sorted_indices[:top_k].tolist()\n",
    "    for gene_index in gene_indices:\n",
    "        if gene_index not in top_indices:\n",
    "            top_indices[0] = gene_index\n",
    "    u_sparse = np.zeros_like(u_hat)\n",
    "    u_sparse[top_indices] = u_hat[top_indices]\n",
    "    return u_sparse, sorted(top_indices)\n",
    "\n",
    "def run_mcmc_horseshoe_learnable_sigma(Sigma_sub, delta_X_sub, draws=1000, tune=1000):\n",
    "    G = Sigma_sub.shape[0]\n",
    "    with pm.Model() as model:\n",
    "        lambda_ = pm.HalfCauchy(\"lambda\", beta=1.0, shape=G)\n",
    "        log_tau = pm.Normal(\"log_tau\", mu=-4, sigma=1)\n",
    "        tau = pm.Deterministic(\"tau\", pm.math.exp(log_tau))\n",
    "        z = pm.Normal(\"z\", 0, 1, shape=G)\n",
    "        u = pm.Deterministic(\"u\", z * tau * lambda_)\n",
    "        log_sigma = pm.Normal(\"log_sigma_obs\", mu=-2, sigma=2)\n",
    "        sigma_obs = pm.Deterministic(\"sigma_obs\", pm.math.exp(log_sigma))\n",
    "        mu_x = pm.math.dot(Sigma_sub, u)\n",
    "        obs = pm.Normal(\"obs\", mu=mu_x, sigma=sigma_obs, observed=delta_X_sub)\n",
    "        trace = pm.sample(draws=draws, tune=tune, target_accept=0.95, max_treedepth=10, chains=8, cores=8, progressbar=True)\n",
    "    return trace\n",
    "\n",
    "def save_u_samples_summary_double_pert(data_path, output_dir=\"u_samples_summaries_double\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    adata, X0, X1 = get_data(0, data_path)\n",
    "    if adata is None:\n",
    "        return None\n",
    "\n",
    "    gene_names = np.array(adata.var_names.tolist())\n",
    "    Sigma = np.cov(X0, rowvar=False)\n",
    "    Sigma_inv = pinv(Sigma)\n",
    "\n",
    "    perturbations = [p for p in adata.obs['perturbation'].unique() if '_' in p]\n",
    "    all_rows = []\n",
    "    dataset_name = os.path.basename(data_path).replace(\".h5ad\", \"\")\n",
    "\n",
    "    for pert in perturbations:\n",
    "        pert_split = pert.split('_')\n",
    "        if len(pert_split) != 2:\n",
    "            continue\n",
    "        g1, g2 = pert_split\n",
    "        gene_indices = [np.where(gene_names == g)[0][0] for g in [g1, g2] if g in gene_names]\n",
    "        if len(gene_indices) < 2:\n",
    "            continue\n",
    "\n",
    "        X1 = adata[adata.obs['perturbation'] == pert].X\n",
    "        X1 = X1.toarray() if issparse(X1) else X1\n",
    "        delta_X = compute_average_response(X0, X1)\n",
    "\n",
    "        u_hat = Sigma_inv @ delta_X\n",
    "        abs_u = np.abs(u_hat)\n",
    "        sorted_indices = np.argsort(-abs_u)\n",
    "        ranks = {idx: rank for rank, idx in enumerate(sorted_indices)}\n",
    "\n",
    "        print(f\"↳ {pert}:\")\n",
    "        for g in [g1, g2]:\n",
    "            idx = np.where(gene_names == g)[0][0]\n",
    "            print(f\"   - Gene {g} ranked #{ranks[idx]} with |u_hat| = {abs_u[idx]:.4f}\")\n",
    "\n",
    "        u_sparse, sparse_indices = compute_sparse_perturbation(Sigma_inv, delta_X, gene_indices, top_k=200)\n",
    "        Sigma_sub = Sigma[np.ix_(sparse_indices, sparse_indices)]\n",
    "        delta_X_sub = delta_X[sparse_indices]\n",
    "\n",
    "        try:\n",
    "            trace = run_mcmc_horseshoe_learnable_sigma(Sigma_sub, delta_X_sub)\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP] {pert} due to MCMC error: {e}\")\n",
    "            continue\n",
    "\n",
    "        u_samples = trace.posterior['u'].stack(sample=(\"chain\", \"draw\")).values\n",
    "        u_mean_local = np.mean(u_samples, axis=1)\n",
    "        u_std_local = np.std(u_samples, axis=1)\n",
    "        pip_local = np.mean(np.abs(u_samples) > 0.05, axis=1)\n",
    "\n",
    "        for j, idx in enumerate(sparse_indices):\n",
    "            row = {\n",
    "                \"Gene\": gene_names[idx],\n",
    "                \"Perturbation\": pert,\n",
    "                \"IsTruePerturbation\": int(idx in gene_indices),\n",
    "                \"PIP\": pip_local[j],\n",
    "                \"U_Mean\": u_mean_local[j],\n",
    "                \"U_Std\": u_std_local[j],\n",
    "                \"U_Samples\": u_samples[j].tolist() if idx in gene_indices else None\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "\n",
    "        df_pert = pd.DataFrame([row for row in all_rows if row[\"Perturbation\"] == pert])\n",
    "        if df_pert.empty:\n",
    "            continue\n",
    "\n",
    "        true_gene_list = [gene_names[i] for i in gene_indices]\n",
    "        gene_to_idx = {row[\"Gene\"]: i for i, row in df_pert.iterrows()}\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.scatter(range(len(df_pert)), df_pert[\"PIP\"], alpha=0.5)\n",
    "        for g in true_gene_list:\n",
    "            idx = gene_to_idx.get(g)\n",
    "            if idx is not None:\n",
    "                plt.scatter(idx, df_pert.loc[idx, \"PIP\"], color='red', s=60, label=g)\n",
    "        plt.title(f\"PIP for {pert} — {dataset_name}\")\n",
    "        plt.xlabel(\"Sparse Gene Index\")\n",
    "        plt.ylabel(\"Posterior Inclusion Probability\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/{dataset_name}_{pert}_PIP.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        x = range(len(df_pert))\n",
    "        y = df_pert[\"U_Mean\"]\n",
    "        yerr = df_pert[\"U_Std\"]\n",
    "        plt.errorbar(x, y, yerr=yerr, fmt='o', alpha=0.7, capsize=3)\n",
    "        for g in true_gene_list:\n",
    "            idx = gene_to_idx.get(g)\n",
    "            if idx is not None:\n",
    "                plt.scatter(idx, df_pert.loc[idx, \"U_Mean\"], color='red', s=60, label=g)\n",
    "        plt.title(f\"Posterior mean ± std for {pert} — {dataset_name}\")\n",
    "        plt.xlabel(\"Sparse Gene Index\")\n",
    "        plt.ylabel(\"Posterior Mean of $u$\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/{dataset_name}_{pert}_UMean.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        for g in true_gene_list:\n",
    "            idx = gene_to_idx.get(g)\n",
    "            if idx is not None:\n",
    "                u_samples = df_pert.loc[idx, \"U_Samples\"]\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.hist(u_samples, bins=50, density=True, alpha=0.7)\n",
    "                plt.axvline(0, color='black', linestyle='--')\n",
    "                plt.yscale('log')\n",
    "                plt.title(f\"Posterior of $u_{{{g}}}$ — {pert} — {dataset_name}\")\n",
    "                plt.xlabel(\"Value of $u$\")\n",
    "                plt.ylabel(\"Density\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{output_dir}/{dataset_name}_{pert}_{g}_posterior.png\")\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "    summary_df = pd.DataFrame(all_rows)\n",
    "    save_path = os.path.join(output_dir, os.path.basename(data_path).replace(\".h5ad\", \"_usamples_double.pkl\"))\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(summary_df, f)\n",
    "    print(f\"Saved summary + plots for: {data_path}\")\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d3aec-6bc3-4784-8fbc-1446f1ce4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_pert_paths = [\n",
    "    \"TianKampmann2019_day7neuron.h5ad\",\n",
    "    \"NormanWeissman2019_filtered.h5ad\"\n",
    "]\n",
    "\n",
    "for path in double_pert_paths:\n",
    "    path = os.path.join(data_dir, path)\n",
    "    save_u_samples_summary_double_pert(data_path=path, output_dir=\"u_samples_summaries_double\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a7e15-0b73-49f2-b68f-bab47e06e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this produces the figures for fig4 supplement \n",
    "# runs inference and plots a few specific double perturbations\n",
    "\n",
    "\n",
    "import pytensor\n",
    "pytensor.config.cxx = \"\"\n",
    "pytensor.config.mode = \"NUMBA\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import pinv\n",
    "from scipy.sparse import issparse\n",
    "import pymc as pm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def compute_average_response(X0, X1):\n",
    "    return X1.mean(axis=0) - X0.mean(axis=0)\n",
    "\n",
    "def compute_sparse_perturbation(Sigma_inv, delta_X, gene_indices, top_k=200):\n",
    "    u_hat = Sigma_inv @ delta_X\n",
    "    sorted_indices = np.argsort(np.abs(u_hat))[::-1]\n",
    "    top_indices = sorted_indices[:top_k].tolist()\n",
    "    for gene_index in gene_indices:\n",
    "        if gene_index not in top_indices:\n",
    "            top_indices[0] = gene_index\n",
    "    u_sparse = np.zeros_like(u_hat)\n",
    "    u_sparse[top_indices] = u_hat[top_indices]\n",
    "    return u_sparse, sorted(top_indices)\n",
    "\n",
    "def run_mcmc_horseshoe_learnable_sigma(Sigma_sub, delta_X_sub, draws=1000, tune=1000):\n",
    "    G = Sigma_sub.shape[0]\n",
    "    with pm.Model() as model:\n",
    "        lambda_ = pm.HalfCauchy(\"lambda\", beta=1.0, shape=G)\n",
    "        log_tau = pm.Normal(\"log_tau\", mu=-4, sigma=1)\n",
    "        tau = pm.Deterministic(\"tau\", pm.math.exp(log_tau))\n",
    "        z = pm.Normal(\"z\", 0, 1, shape=G)\n",
    "        u = pm.Deterministic(\"u\", z * tau * lambda_)\n",
    "        log_sigma = pm.Normal(\"log_sigma_obs\", mu=-2, sigma=2)\n",
    "        sigma_obs = pm.Deterministic(\"sigma_obs\", pm.math.exp(log_sigma))\n",
    "        mu_x = pm.math.dot(Sigma_sub, u)\n",
    "        obs = pm.Normal(\"obs\", mu=mu_x, sigma=sigma_obs, observed=delta_X_sub)\n",
    "        trace = pm.sample(draws=draws, tune=tune, target_accept=0.95, max_treedepth=10, chains=8, cores=8)\n",
    "    return trace\n",
    "\n",
    "def run_inference_for_selected_double_perturbations(data_path, selected_dps, output_dir=\"fig4supplement\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    adata, X0, _ = get_data(0, data_path)\n",
    "    if adata is None:\n",
    "        print(f\"Could not load valid data from: {data_path}\")\n",
    "        return None\n",
    "\n",
    "    gene_names = np.array(adata.var_names.tolist())\n",
    "    Sigma = np.cov(X0, rowvar=False)\n",
    "    Sigma_inv = pinv(Sigma)\n",
    "    dataset_name = os.path.basename(data_path).replace(\".h5ad\", \"\")\n",
    "    all_rows = []\n",
    "\n",
    "    for pert in selected_dps:\n",
    "        if pert not in adata.obs['perturbation'].unique():\n",
    "            print(f\"Perturbation {pert} not found in dataset.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            g1, g2 = pert.split(\"_\")\n",
    "        except:\n",
    "            print(f\"Invalid format for {pert}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        gene_indices = [np.where(gene_names == g)[0][0] for g in [g1, g2] if g in gene_names]\n",
    "        if len(gene_indices) < 2:\n",
    "            print(f\"One or both genes not found in var_names for {pert}\")\n",
    "            continue\n",
    "\n",
    "        X1 = adata[adata.obs['perturbation'] == pert].X\n",
    "        X1 = X1.toarray() if issparse(X1) else X1\n",
    "        delta_X = compute_average_response(X0, X1)\n",
    "\n",
    "        u_hat = Sigma_inv @ delta_X\n",
    "        abs_u = np.abs(u_hat)\n",
    "        sorted_indices = np.argsort(-abs_u)\n",
    "        ranks = {idx: rank for rank, idx in enumerate(sorted_indices)}\n",
    "\n",
    "        print(f\"↳ {pert}:\")\n",
    "        for g in [g1, g2]:\n",
    "            idx = np.where(gene_names == g)[0][0]\n",
    "            print(f\"   - Gene {g} ranked #{ranks[idx]} with |u_hat| = {abs_u[idx]:.4f}\")\n",
    "\n",
    "        u_sparse, sparse_indices = compute_sparse_perturbation(Sigma_inv, delta_X, gene_indices, top_k=200)\n",
    "        Sigma_sub = Sigma[np.ix_(sparse_indices, sparse_indices)]\n",
    "        delta_X_sub = delta_X[sparse_indices]\n",
    "\n",
    "        try:\n",
    "            trace = run_mcmc_horseshoe_learnable_sigma(Sigma_sub, delta_X_sub)\n",
    "        except Exception as e:\n",
    "            print(f\"MCMC failed for {pert}: {e}\")\n",
    "            continue\n",
    "\n",
    "        u_samples = trace.posterior['u'].stack(sample=(\"chain\", \"draw\")).values\n",
    "        u_mean_local = np.mean(u_samples, axis=1)\n",
    "        u_std_local = np.std(u_samples, axis=1)\n",
    "        pip_local = np.mean(np.abs(u_samples) > 0.05, axis=1)\n",
    "\n",
    "        for j, idx in enumerate(sparse_indices):\n",
    "            row = {\n",
    "                \"Gene\": gene_names[idx],\n",
    "                \"Perturbation\": pert,\n",
    "                \"IsTruePerturbation\": int(idx in gene_indices),\n",
    "                \"PIP\": pip_local[j],\n",
    "                \"U_Mean\": u_mean_local[j],\n",
    "                \"U_Std\": u_std_local[j],\n",
    "                \"U_Samples\": u_samples[j].tolist() if idx in gene_indices else None\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "\n",
    "        df_pert = pd.DataFrame([row for row in all_rows if row[\"Perturbation\"] == pert])\n",
    "        if df_pert.empty:\n",
    "            continue\n",
    "\n",
    "        true_gene_list = [gene_names[i] for i in gene_indices]\n",
    "        gene_to_idx = {row[\"Gene\"]: i for i, row in df_pert.iterrows()}\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.scatter(range(len(df_pert)), df_pert[\"PIP\"], alpha=0.5)\n",
    "        for g in true_gene_list:\n",
    "            idx = gene_to_idx.get(g)\n",
    "            if idx is not None:\n",
    "                plt.scatter(idx, df_pert.loc[idx, \"PIP\"], color='red', s=60, label=g)\n",
    "        plt.title(f\"PIP for {pert} — {dataset_name}\")\n",
    "        plt.xlabel(\"Sparse Gene Index\")\n",
    "        plt.ylabel(\"Posterior Inclusion Probability\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/{dataset_name}_{pert}_PIP.svg\")\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        x = range(len(df_pert))\n",
    "        y = df_pert[\"U_Mean\"]\n",
    "        yerr = df_pert[\"U_Std\"]\n",
    "        plt.errorbar(x, y, yerr=yerr, fmt='o', alpha=0.7, capsize=3)\n",
    "        for g in true_gene_list:\n",
    "            idx = gene_to_idx.get(g)\n",
    "            if idx is not None:\n",
    "                plt.scatter(idx, df_pert.loc[idx, \"U_Mean\"], color='red', s=60, label=g)\n",
    "        plt.title(f\"Posterior mean ± std for {pert} — {dataset_name}\")\n",
    "        plt.xlabel(\"Sparse Gene Index\")\n",
    "        plt.ylabel(\"Posterior Mean of $u$\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/{dataset_name}_{pert}_UMean.svg\")\n",
    "        plt.close()\n",
    "\n",
    "        for g in true_gene_list:\n",
    "            idx = gene_to_idx.get(g)\n",
    "            if idx is not None:\n",
    "                u_samples = df_pert.loc[idx, \"U_Samples\"]\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.hist(u_samples, bins=50, density=True, alpha=0.7)\n",
    "                plt.axvline(0, color='black', linestyle='--')\n",
    "                plt.yscale('log')\n",
    "                plt.title(f\"Posterior of $u_{{{g}}}$ — {pert} — {dataset_name}\")\n",
    "                plt.xlabel(\"Value of $u$\")\n",
    "                plt.ylabel(\"Density\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{output_dir}/{dataset_name}_{pert}_{g}_posterior.svg\")\n",
    "                plt.close()\n",
    "\n",
    "    summary_df = pd.DataFrame(all_rows)\n",
    "    save_path = os.path.join(output_dir, f\"{dataset_name}_selected_usamples.pkl\")\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(summary_df, f)\n",
    "    print(f\"Saved summary and plots to: {output_dir}\")\n",
    "    return summary_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Call for selected perturbations ===\n",
    "selected_double_perturbations = ['ZC3HAV1_HOXC13', 'ZC3HAV1_CEBPE', 'SGK1_S1PR2', 'PTPN12_SNAIL1']\n",
    "run_inference_for_selected_double_perturbations(\n",
    "    data_path=os.path.join(data_dir, \"NormanWeissman2019_filtered.h5ad\"),\n",
    "    selected_dps=selected_double_perturbations,\n",
    "    output_dir=\"fig4supplement\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5fd0dd-b195-4d6f-9349-2ec2eb6cb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the inference data generated above to produce rocauc curves for \n",
    "# FIG 4 H\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, average_precision_score\n",
    "\n",
    "# === Load summaries ===\n",
    "def load_u_samples_summary(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "summary_files = sorted(glob.glob(\"u_samples_summaries_double/*.pkl\"))\n",
    "print(f\"Found {len(summary_files)} summaries.\")\n",
    "\n",
    "dfs = []\n",
    "for file in summary_files:\n",
    "    df = load_u_samples_summary(file)\n",
    "    df['Dataset'] = os.path.basename(file).replace(\"_usamples_with_lr.pkl\", \"\")\n",
    "    dfs.append(df)\n",
    "\n",
    "all_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(all_df)} rows total.\")\n",
    "\n",
    "# === ROC AUC for max(|U ± std|) — Per Dataset ===\n",
    "datasets = all_df['Dataset'].unique()\n",
    "roc_data_all = []\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for dataset in datasets:\n",
    "    df_subset = all_df.query(f\"Dataset == '{dataset}'\")\n",
    "    if df_subset['IsTruePerturbation'].sum() == 0:\n",
    "        print(f\"⚠️ Skipping {dataset} (no true perturbations)\")\n",
    "        continue\n",
    "\n",
    "    optimistic = np.abs(df_subset['U_Mean'] + df_subset['U_Std'])\n",
    "    pessimistic = np.abs(df_subset['U_Mean'] - df_subset['U_Std'])\n",
    "    max_up_down = np.maximum(optimistic, pessimistic)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(df_subset['IsTruePerturbation'], max_up_down)\n",
    "    roc_auc = roc_auc_score(df_subset['IsTruePerturbation'], max_up_down)\n",
    "    ap_score = average_precision_score(df_subset['IsTruePerturbation'], max_up_down)\n",
    "\n",
    "    plt.plot(fpr, tpr, label=f\"{dataset} (AUC = {roc_auc:.2f})\", linestyle='-.')\n",
    "\n",
    "    roc_data_all.append({\n",
    "        \"Dataset\": dataset,\n",
    "        \"Metric\": \"Max_Abs_U_plus_minus_Std\",\n",
    "        \"ROC_AUC\": roc_auc,\n",
    "        \"AveragePrecision\": ap_score\n",
    "    })\n",
    "\n",
    "# === Finalize individual plot ===\n",
    "plt.plot([0, 1], [0, 1], 'k:', lw=1)\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC Curves per Dataset: max(|U ± std|)', fontsize=18)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"roc_curve_individual_maxUstd.svg\")\n",
    "plt.show()\n",
    "\n",
    "# === Combined ROC Curve (Separate Plot) ===\n",
    "optimistic_all = np.abs(all_df['U_Mean'] + all_df['U_Std'])\n",
    "pessimistic_all = np.abs(all_df['U_Mean'] - all_df['U_Std'])\n",
    "max_up_down_all = np.maximum(optimistic_all, pessimistic_all)\n",
    "\n",
    "fpr_comb, tpr_comb, _ = roc_curve(all_df['IsTruePerturbation'], max_up_down_all)\n",
    "roc_auc_comb = roc_auc_score(all_df['IsTruePerturbation'], max_up_down_all)\n",
    "ap_score_comb = average_precision_score(all_df['IsTruePerturbation'], max_up_down_all)\n",
    "\n",
    "# Save combined stats\n",
    "roc_data_all.append({\n",
    "    \"Dataset\": \"Combined\",\n",
    "    \"Metric\": \"Max_Abs_U_plus_minus_Std\",\n",
    "    \"ROC_AUC\": roc_auc_comb,\n",
    "    \"AveragePrecision\": ap_score_comb\n",
    "})\n",
    "\n",
    "# Plot combined\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_comb, tpr_comb, color='black', label=f\"Combined (AUC = {roc_auc_comb:.2f})\", linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('ROC Curve (Combined): max(|U ± std|)', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"roc_curve_combined_maxUstd.svg\")\n",
    "plt.show()\n",
    "\n",
    "# === Save summary ===\n",
    "roc_summary_full_df = pd.DataFrame(roc_data_all)\n",
    "roc_summary_full_df.to_csv(\"rocauc_summary_full_maxUstd_per_dataset.csv\", index=False)\n",
    "print(\"Saved summary to 'rocauc_summary_full_maxUstd_per_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5af99-ff78-40f8-b8f7-93685644d80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a063f93-4cf7-4dd6-91c4-66ccca3af388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 (FIG3 plots and supps) BELOW HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107fd21-67fb-44e2-8052-4d3562213a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is code to calcualte different R2 metric for each double perturbation\n",
    "# FIG 3N,O\n",
    "\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import issparse\n",
    "from scipy.linalg import pinv\n",
    "import os\n",
    "\n",
    "# ---------- Helper Functions ----------\n",
    "\n",
    "def is_outlier(adata, column, nmads):\n",
    "    vals = adata.obs[column]\n",
    "    median = np.median(vals)\n",
    "    mad = np.median(np.abs(vals - median))\n",
    "    threshold = nmads * mad\n",
    "    return (vals > median + threshold) | (vals < median - threshold)\n",
    "\n",
    "def quality_control_filter(adata, percent_threshold=20, nmads=5, mt_nmads=5, mt_per=20):\n",
    "    adata.var_names = adata.var_names.astype(str)\n",
    "    adata.var_names_make_unique()\n",
    "    adata.var['mt'] = adata.var_names.str.startswith('MT-')\n",
    "    adata.var['ribo'] = adata.var_names.str.startswith(('RPS', 'RPL'))\n",
    "    adata.var['hb'] = adata.var_names.str.contains('^HB[^(P)]')\n",
    "\n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt', 'ribo', 'hb'],\n",
    "                               inplace=True, percent_top=[percent_threshold], log1p=True)\n",
    "\n",
    "    adata.obs['outlier'] = (is_outlier(adata, 'log1p_total_counts', nmads) |\n",
    "                             is_outlier(adata, 'log1p_n_genes_by_counts', nmads) |\n",
    "                             is_outlier(adata, 'pct_counts_in_top_20_genes', nmads))\n",
    "    adata.obs['mt_outlier'] = is_outlier(adata, 'pct_counts_mt', mt_nmads) | (adata.obs['pct_counts_mt'] > mt_per)\n",
    "\n",
    "    adata = adata[adata.obs['n_genes_by_counts'] > 200]\n",
    "    gene_counts = np.sum(adata.X > 0, axis=0)\n",
    "    genes_to_keep = np.array(gene_counts).flatten() >= 3\n",
    "    adata = adata[:, genes_to_keep]\n",
    "    adata = adata[(~adata.obs.outlier) & (~adata.obs.mt_outlier)]\n",
    "\n",
    "    return adata\n",
    "\n",
    "def get_double_perts(adata):\n",
    "    adata = adata.copy()\n",
    "    adata.obs['n_perts'] = adata.obs.perturbation.str.split('_').apply(len)\n",
    "    double_perts = adata.obs[adata.obs['n_perts'] == 2].perturbation.unique()\n",
    "    pert_counts = adata.obs['perturbation'].value_counts()\n",
    "\n",
    "    dp_list = []\n",
    "    for dp in double_perts:\n",
    "        dp_count = pert_counts.get(dp, 0)\n",
    "        g1, g2 = dp.split('_')\n",
    "        g1_count = pert_counts.get(g1, 0)\n",
    "        g2_count = pert_counts.get(g2, 0)\n",
    "        if min(g1_count, g2_count, dp_count) > 50:\n",
    "            dp_list.append(dp)\n",
    "    return dp_list\n",
    "\n",
    "def get_data(selected_index, data_path):\n",
    "    adata = ad.read_h5ad(data_path)\n",
    "    print(f\"Original data shape: {adata.shape}\")\n",
    "    adata = quality_control_filter(adata)\n",
    "\n",
    "    if issparse(adata.X):\n",
    "        total_counts = adata.X.sum(axis=1).A1\n",
    "    else:\n",
    "        total_counts = adata.X.sum(axis=1)\n",
    "    adata = adata[total_counts >= 500].copy()\n",
    "\n",
    "    pert_counts = adata.obs['perturbation'].value_counts()\n",
    "    valid_perts = pert_counts[pert_counts >= 100].index.tolist()\n",
    "    adata = adata[adata.obs['perturbation'].isin(valid_perts)]\n",
    "    print(f\"Number of valid perturbations: {len(valid_perts)}\")\n",
    "\n",
    "    dp_list = get_double_perts(adata)\n",
    "    if len(dp_list) == 0:\n",
    "        print(f\"No double perturbations in {data_path}, skipping.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    if selected_index >= len(dp_list):\n",
    "        print(f\"Selected index {selected_index} exceeds available double perts ({len(dp_list)}), skipping.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    selected_pert_name = dp_list[selected_index]\n",
    "    print('selected_pert_name =', selected_pert_name)\n",
    "\n",
    "    gene_names = adata.var_names.to_numpy()\n",
    "    gene_map = {g: i for i, g in enumerate(gene_names)}\n",
    "    double_pert_genes = set()\n",
    "    for dp in dp_list:\n",
    "        g1, g2 = dp.split('_')\n",
    "        if g1 in gene_map:\n",
    "            double_pert_genes.add(gene_map[g1])\n",
    "        if g2 in gene_map:\n",
    "            double_pert_genes.add(gene_map[g2])\n",
    "\n",
    "    control_data0 = adata[adata.obs['perturbation'] == 'control']\n",
    "    X_dense = control_data0.X.toarray() if issparse(control_data0.X) else control_data0.X\n",
    "    gene_means = X_dense.mean(axis=0)\n",
    "    valid_genes = np.where(gene_means >= 1.0)[0]\n",
    "    all_valid_genes = np.unique(np.concatenate([valid_genes, sorted(double_pert_genes)]))\n",
    "    adata = adata[:, all_valid_genes]\n",
    "\n",
    "    control_data0 = adata[adata.obs['perturbation'] == 'control']\n",
    "    selected_pert_data = adata[adata.obs['perturbation'] == selected_pert_name]\n",
    "    n_samples = min(control_data0.shape[0], selected_pert_data.shape[0])\n",
    "    control_data = control_data0[:n_samples]\n",
    "    selected_pert_data = selected_pert_data[:n_samples]\n",
    "\n",
    "    X0_full = control_data0.X.toarray() if issparse(control_data0.X) else control_data0.X\n",
    "    X0 = control_data.X.toarray() if issparse(control_data.X) else control_data.X\n",
    "    X1 = selected_pert_data.X.toarray() if issparse(selected_pert_data.X) else selected_pert_data.X\n",
    "\n",
    "    print('Shapes X0, X1:', X0.shape, X1.shape)\n",
    "    return adata, X0_full, X0, X1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def full_double_perturbation_analysis(data_path, save_dir=\"r2_histograms\"):\n",
    "    adata, X0, _, _ = get_data(0, data_path)\n",
    "    if adata is None:\n",
    "        print('No double perts in dataset ', data_path)\n",
    "        return None\n",
    "\n",
    "    def get_gene_index(gene_name, gene_names):\n",
    "        indices = np.where(gene_names == gene_name)[0]\n",
    "        return int(indices[0]) if indices.size > 0 else None\n",
    "\n",
    "    def compute_covariance(X):\n",
    "        return np.cov(X, rowvar=False)\n",
    "\n",
    "    def compute_average_response(X0, X1):\n",
    "        return X1.mean(axis=0) - X0.mean(axis=0)\n",
    "\n",
    "    gene_names = np.array(adata.var_names.tolist())\n",
    "    X0_dense = X0.toarray() if issparse(X0) else X0\n",
    "    Sigma = compute_covariance(X0_dense)\n",
    "\n",
    "    # Null model\n",
    "    X0_shuffled = X0.copy()\n",
    "    for g in range(X0.shape[1]):\n",
    "        np.random.shuffle(X0_shuffled[:, g])\n",
    "    Sigma_S = compute_covariance(X0_shuffled)\n",
    "\n",
    "    perturbations = [p for p in adata.obs['perturbation'].unique() if \"_\" in p]\n",
    "    epsilon = 1e-8\n",
    "    R2_real, R2_null, R2_A_list, R2_B_list, R2_additive, pert_names = [], [], [], [], [], []\n",
    "\n",
    "    for pert in perturbations:\n",
    "        pert_A, pert_B = pert.split(\"_\")\n",
    "        gene_idx_A = get_gene_index(pert_A, gene_names)\n",
    "        gene_idx_B = get_gene_index(pert_B, gene_names)\n",
    "\n",
    "        if gene_idx_A is None or gene_idx_B is None:\n",
    "            print(f\"Skipping {pert} (missing gene index)\")\n",
    "            continue\n",
    "\n",
    "        # Load expression for double and singles\n",
    "        X1_AB = adata[adata.obs['perturbation'] == pert].X\n",
    "        X1_A = adata[adata.obs['perturbation'] == pert_A].X\n",
    "        X1_B = adata[adata.obs['perturbation'] == pert_B].X\n",
    "\n",
    "        X1_AB = X1_AB.toarray() if issparse(X1_AB) else X1_AB\n",
    "        X1_A = X1_A.toarray() if issparse(X1_A) else X1_A\n",
    "        X1_B = X1_B.toarray() if issparse(X1_B) else X1_B\n",
    "\n",
    "        # ΔX\n",
    "        delta_X_AB = compute_average_response(X0_dense, X1_AB)\n",
    "        delta_X_A = compute_average_response(X0_dense, X1_A)\n",
    "        delta_X_B = compute_average_response(X0_dense, X1_B)\n",
    "\n",
    "        # Fit using real covariance\n",
    "        Sigma_AB = Sigma[:, [gene_idx_A, gene_idx_B]]\n",
    "        u_AB, _, _, _ = np.linalg.lstsq(Sigma_AB, delta_X_AB, rcond=None)\n",
    "        pred_AB = Sigma_AB @ u_AB\n",
    "\n",
    "        Sigma_A = Sigma[:, [gene_idx_A]]\n",
    "        Sigma_B = Sigma[:, [gene_idx_B]]\n",
    "\n",
    "        u_A, _, _, _ = np.linalg.lstsq(Sigma_A, delta_X_A, rcond=None)\n",
    "        u_B, _, _, _ = np.linalg.lstsq(Sigma_B, delta_X_B, rcond=None)\n",
    "\n",
    "        pred_A = Sigma_A @ u_A\n",
    "        pred_B = Sigma_B @ u_B\n",
    "        pred_add = pred_A + pred_B\n",
    "\n",
    "        # Fit using null covariance\n",
    "        Sigma_AB_null = Sigma_S[:, [gene_idx_A, gene_idx_B]]\n",
    "        u_null, _, _, _ = np.linalg.lstsq(Sigma_AB_null, delta_X_AB, rcond=None)\n",
    "        pred_null = Sigma_AB_null @ u_null\n",
    "\n",
    "        # Filter for informative genes\n",
    "        mean_diff = np.abs(X1_AB.mean(axis=0) - X0_dense.mean(axis=0))\n",
    "        valid_idx = np.where(mean_diff > 0)[0]\n",
    "        if len(valid_idx) == 0:\n",
    "            print(f\"Skipping {pert} (no gene change)\")\n",
    "            continue\n",
    "\n",
    "        # Filter all deltas and predictions\n",
    "        delta = delta_X_AB[valid_idx]\n",
    "        pred_AB = pred_AB[valid_idx]\n",
    "        pred_null = pred_null[valid_idx]\n",
    "        pred_A = pred_A[valid_idx]\n",
    "        pred_B = pred_B[valid_idx]\n",
    "        pred_add = pred_add[valid_idx]\n",
    "\n",
    "        def R2(y_true, y_pred):\n",
    "            return 1.0 - np.sqrt(np.sum((y_true - y_pred) ** 2)) / np.sqrt(np.sum(y_true ** 2) + epsilon)\n",
    "\n",
    "        # Compute all R²s\n",
    "        R2_real.append(R2(delta, pred_AB))\n",
    "        R2_null.append(R2(delta, pred_null))\n",
    "        R2_A_list.append(R2(delta, pred_A))\n",
    "        R2_B_list.append(R2(delta, pred_B))\n",
    "        R2_additive.append(R2(delta, pred_add))\n",
    "        pert_names.append(pert)\n",
    "\n",
    "    # === Save & Plot ===\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    base = os.path.basename(data_path).replace('.h5ad', '')\n",
    "    np.savez(\n",
    "        os.path.join(save_dir, f\"R2_{base}_doublepert.npz\"),\n",
    "        perturbations=np.array(pert_names),\n",
    "        R2_real=np.array(R2_real),\n",
    "        R2_null=np.array(R2_null),\n",
    "        R2_A=np.array(R2_A_list),\n",
    "        R2_B=np.array(R2_B_list),\n",
    "        R2_additive=np.array(R2_additive)\n",
    "    )\n",
    "\n",
    "\n",
    "        # Histogram with 5 R² distributions\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    bins = np.linspace(0, 1, 40)\n",
    "\n",
    "    plt.hist(R2_real, bins=bins, alpha=0.7, label='Double Pert. R²', density=True)\n",
    "    plt.hist(R2_additive, bins=bins, alpha=0.7, label='Additive (A + B) R²', density=True)\n",
    "    plt.hist(R2_A_list, bins=bins, alpha=0.5, label='Single A R²', density=True)\n",
    "    plt.hist(R2_B_list, bins=bins, alpha=0.5, label='Single B R²', density=True)\n",
    "    plt.hist(R2_null, bins=bins, alpha=0.3, label='Null R² (shuffled)', density=True)\n",
    "\n",
    "    plt.xlabel(\"R² Score\", fontsize=18)\n",
    "    plt.ylabel(\"Density\", fontsize=18)\n",
    "    plt.title(f\"Double Perturbation R² — {base}\", fontsize=20)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"double_r2_hist_{base}.png\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved 5-R² histogram to {save_path}\")\n",
    "\n",
    "    return {\n",
    "        \"perturbations\": pert_names,\n",
    "        \"R2_real\": R2_real,\n",
    "        \"R2_null\": R2_null,\n",
    "        \"R2_A\": R2_A_list,\n",
    "        \"R2_B\": R2_B_list,\n",
    "        \"R2_additive\": R2_additive\n",
    "    }\n",
    "\n",
    "\n",
    "datapaths = [\n",
    "    \"NormanWeissman2019_filtered.h5ad\",\n",
    "    \"TianKampmann2019_day7neuron.h5ad\",\n",
    "]\n",
    "\n",
    "for path in datapaths:\n",
    "    path = os.path.join(data_dir, path)\n",
    "    result = full_double_perturbation_analysis(path)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "save_dir = os.path.join(out_dir, \"r2_histograms\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Collect R² mean per dataset ===\n",
    "mean_r2_list = []\n",
    "for path in datapaths:\n",
    "    base = os.path.basename(path).replace(\".h5ad\", \"\")\n",
    "    npz_path = os.path.join(save_dir, f\"R2_{base}_doublepert.npz\")\n",
    "\n",
    "    if not os.path.exists(npz_path):\n",
    "        print(f\"Running analysis on {base}...\")\n",
    "        result = full_double_perturbation_analysis(path, save_dir=save_dir)\n",
    "        if result is None:\n",
    "            continue\n",
    "    else:\n",
    "        result = dict(np.load(npz_path, allow_pickle=True))\n",
    "\n",
    "    mean_r2 = {\n",
    "        key: np.mean(result[key]) for key in\n",
    "        [\"R2_null\", \"R2_A\", \"R2_B\", \"R2_additive\", \"R2_real\"]\n",
    "    }\n",
    "    mean_r2_list.append(mean_r2)\n",
    "\n",
    "# === Compute average across datasets ===\n",
    "conditions = [\"R2_null\", \"R2_A\", \"R2_B\", \"R2_additive\", \"R2_real\"]\n",
    "condition_labels = [\"Shuffled\", \"Single A\", \"Single B\", \"Additive\", \"True Σ\"]\n",
    "\n",
    "mean_r2_avg = {\n",
    "    cond: np.mean([m[cond] for m in mean_r2_list]) for cond in conditions\n",
    "}\n",
    "yvals = [mean_r2_avg[c] for c in conditions]\n",
    "\n",
    "# === Bar plot of averaged R²s ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(condition_labels, yvals, color='skyblue')\n",
    "plt.ylabel(\"Mean R² (avg. across datasets)\", fontsize=16)\n",
    "plt.title(\"Average R² per Method\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"mean_r2_avg_barplot.svg\"))\n",
    "plt.close()\n",
    "\n",
    "# === Line + point plot of averaged R²s ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(condition_labels, yvals, marker='o', linewidth=2, markersize=8, color='crimson')\n",
    "plt.ylabel(\"Mean R² (avg. across datasets)\", fontsize=16)\n",
    "plt.title(\"Average R² per Method\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"mean_r2_avg_lineplot.svg\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02ec6d-7ebf-4021-bdde-e8b6946a002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 histograms for individual datasets \n",
    "# Fig S3D\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Paths ===\n",
    "datapaths = [\n",
    "    \"NormanWeissman2019_filtered.h5ad\",\n",
    "    \"TianKampmann2019_day7neuron.h5ad\",\n",
    "]\n",
    "save_dir = os.path.join(out_dir, \"r2_histograms\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Plot one histogram per dataset ===\n",
    "for p in datapaths:\n",
    "    path = os.path.join(data_dir, p)\n",
    "    base = os.path.basename(path).replace(\".h5ad\", \"\")\n",
    "    npz_path = os.path.join(save_dir, f\"R2_{base}_doublepert.npz\")\n",
    "    if not os.path.exists(npz_path):\n",
    "        print(f\"Missing: {npz_path}\")\n",
    "        continue\n",
    "\n",
    "    data = dict(np.load(npz_path, allow_pickle=True))\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    bins = np.linspace(0, 1, 40)\n",
    "\n",
    "    plt.hist(data[\"R2_real\"], bins=bins, alpha=0.7, label='Double Pert. R²', density=True)\n",
    "    plt.hist(data[\"R2_additive\"], bins=bins, alpha=0.7, label='Additive R²', density=True)\n",
    "    plt.hist(data[\"R2_A\"], bins=bins, alpha=0.5, label='Single A R²', density=True)\n",
    "    plt.hist(data[\"R2_B\"], bins=bins, alpha=0.5, label='Single B R²', density=True)\n",
    "    plt.hist(data[\"R2_null\"], bins=bins, alpha=0.3, label='Shuffled R²', density=True)\n",
    "\n",
    "    plt.xlabel(\"R² Score\", fontsize=16)\n",
    "    plt.ylabel(\"Density\", fontsize=16)\n",
    "    plt.title(f\"Double Perturbation R² — {base}\", fontsize=18)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(save_dir, f\"double_r2_hist_{base}.svg\"))\n",
    "    plt.close()\n",
    "    print(f\"Saved SVG for {base}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2c3a5-983d-409f-8e19-48cdcb742d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pvalue stuff for histograms FIG 3 N\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# === Paths and setup ===\n",
    "datapaths = [\n",
    "    \"NormanWeissman2019_filtered.h5ad\",\n",
    "    \"TianKampmann2019_day7neuron.h5ad\",\n",
    "]\n",
    "save_dir = \"r2_histograms\"\n",
    "\n",
    "# === Accumulate R² data across datasets ===\n",
    "r2_all = {\n",
    "    \"Shuffled\": [],\n",
    "    \"Single A\": [],\n",
    "    \"Single B\": [],\n",
    "    \"Additive\": [],\n",
    "    \"True Σ\": [],\n",
    "}\n",
    "\n",
    "# === Load data ===\n",
    "for path in datapaths:\n",
    "    base = os.path.basename(path).replace(\".h5ad\", \"\")\n",
    "    npz_path = os.path.join(save_dir, f\"R2_{base}_doublepert.npz\")\n",
    "\n",
    "    if not os.path.exists(npz_path):\n",
    "        raise FileNotFoundError(f\"{npz_path} not found. You must run the analysis first.\")\n",
    "\n",
    "    data = dict(np.load(npz_path, allow_pickle=True))\n",
    "    r2_all[\"Shuffled\"].extend(data[\"R2_null\"])\n",
    "    r2_all[\"Single A\"].extend(data[\"R2_A\"])\n",
    "    r2_all[\"Single B\"].extend(data[\"R2_B\"])\n",
    "    r2_all[\"Additive\"].extend(data[\"R2_additive\"])\n",
    "    r2_all[\"True Σ\"].extend(data[\"R2_real\"])\n",
    "\n",
    "# === Define comparison pairs ===\n",
    "comparison_pairs = [\n",
    "    (\"Shuffled\", \"Single A\"),\n",
    "    (\"Shuffled\", \"Single B\"),\n",
    "    (\"Shuffled\", \"Additive\"),\n",
    "    (\"Single A\", \"Single B\"),\n",
    "    (\"Additive\", \"True Σ\"),\n",
    "]\n",
    "\n",
    "# === Run KS tests ===\n",
    "print(\"Kolmogorov–Smirnov test p-values:\")\n",
    "for name1, name2 in comparison_pairs:\n",
    "    r2_1 = np.array(r2_all[name1])\n",
    "    r2_2 = np.array(r2_all[name2])\n",
    "    stat, pval = ks_2samp(r2_1, r2_2)\n",
    "    print(f\"{name1} vs {name2}: KS stat = {stat:.4f}, p = {pval:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d0acb-0a24-4227-957f-8591470ecfa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916164e3-8925-40ec-ab79-ecf8e3a8f2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1db5e9-3b18-41b3-9271-b3c39aa782d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pvalue stuff for FIG 3O\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# === Setup ===\n",
    "datapaths = [\n",
    "    \"NormanWeissman2019_filtered.h5ad\",\n",
    "    \"TianKampmann2019_day7neuron.h5ad\",\n",
    "]\n",
    "save_dir = \"r2_histograms\"\n",
    "\n",
    "# === Collect all perturbation-level R²s ===\n",
    "all_pert_r2s = []\n",
    "\n",
    "for path in datapaths:\n",
    "    base = os.path.basename(path).replace(\".h5ad\", \"\")\n",
    "    npz_path = os.path.join(save_dir, f\"R2_{base}_doublepert.npz\")\n",
    "\n",
    "    if not os.path.exists(npz_path):\n",
    "        raise FileNotFoundError(f\"{npz_path} not found. Run the analysis to generate it.\")\n",
    "    \n",
    "    result = dict(np.load(npz_path, allow_pickle=True))\n",
    "\n",
    "    for i in range(len(result[\"R2_real\"])):\n",
    "        all_pert_r2s.append([\n",
    "            result[\"R2_null\"][i],\n",
    "            result[\"R2_A\"][i],\n",
    "            result[\"R2_B\"][i],\n",
    "            result[\"R2_additive\"][i],\n",
    "            result[\"R2_real\"][i]\n",
    "        ])\n",
    "\n",
    "# === Convert to array ===\n",
    "all_pert_r2s = np.array(all_pert_r2s)\n",
    "\n",
    "# === Define comparisons and expected direction (expect second > first) ===\n",
    "comparison_pairs = [\n",
    "    (\"Shuffled\", \"Single A\", 0, 1),\n",
    "    (\"Shuffled\", \"Single B\", 0, 2),\n",
    "    (\"Shuffled\", \"Additive\", 0, 3),\n",
    "    (\"Additive\", \"True Σ\", 3, 4),\n",
    "]\n",
    "\n",
    "# === Compute Wilcoxon one-sided p-values (alternative = 'less') ===\n",
    "print(\"Wilcoxon signed-rank test (one-sided, alternative='less') p-values:\")\n",
    "for name1, name2, idx1, idx2 in comparison_pairs:\n",
    "    stat, pval = wilcoxon(all_pert_r2s[:, idx1], all_pert_r2s[:, idx2], alternative=\"less\")\n",
    "    print(f\"{name1} < {name2}: p = {pval:.6g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ce306-855f-4eb9-9184-4b33fc4032a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bca639-1ff0-4ddc-8216-b11be6e8d686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c313d0-6d6d-494a-af2e-2ed9368a186b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299f422-433c-4276-b8ee-c2069cf8d6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3c2a5-08ed-454c-b44f-618c66700c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff22ac8-3f30-4bc2-b261-3cef302ce205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34534c33-4a36-4a21-850d-b93c0038eb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430312d6-4189-4666-849f-96bd8141f4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ac073-1976-4352-98bc-0cb9714e1929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c282fa1-5f7f-4aa9-b172-873a09d103aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56aa5da-3521-4f98-866e-067d6f43fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cipher",
   "language": "python",
   "name": "cipher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
